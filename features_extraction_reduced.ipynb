{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import corenlp\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize,wordpunct_tokenize,word_tokenize,RegexpTokenizer,TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os,math,glob,re\n",
    "\n",
    "from collections import Counter\n",
    "import re as regex\n",
    "import contractions\n",
    "import copy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length=9999999\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def regTokenize(text):\n",
    "    tok=RegexpTokenizer('[A-Za-z0-9]*[.]?\\w+')\n",
    "    return tok.tokenize(text) \n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemma(words):\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=WordNetLemmatizer().lemmatize(words[i])\n",
    "    return words\n",
    "\n",
    "def stemming(words):\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=porter_stemmer.stem(words[i])\n",
    "    return words\n",
    "\n",
    "def tweet(words):\n",
    "    tok=TweetTokenizer()\n",
    "    return tok.tokenize(words)\n",
    "\n",
    "def comma(text):\n",
    "    text = \"\".join(c for c in text if c not in ('!','.',':',',','\"','?','(',')'))\n",
    "    return text\n",
    "\n",
    "def getBasicNorm(text):\n",
    "    for i in range(0,len(text)):\n",
    "        text[i]=contract(text[i])\n",
    "        text[i]=lowercase(text[i])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWholeString(data):\n",
    "    string=''\n",
    "    for i in data:\n",
    "        string+=i[0]+' '\n",
    "    return string\n",
    "\n",
    "def getNGrams(corpusSentence):\n",
    "    \n",
    "    corpusSentence=contract(corpusSentence)\n",
    "    corpusSentence=lowercase(corpusSentence)\n",
    "    tokenized=regTokenize(corpusSentence)\n",
    "    \n",
    "    uni=list(ngrams(tokenized,1))\n",
    "    bigram=list(ngrams(tokenized,2))\n",
    "    trigram=list(ngrams(tokenized,3))\n",
    "\n",
    "    print('uni:',len(uni),' bi:',len(bigram),'tri:',len(trigram))\n",
    "    grams=[uni,bigram,trigram]\n",
    "    \n",
    "    return grams\n",
    "\n",
    "\n",
    "def postagging(corpusSentence):\n",
    "    \n",
    "    corpusSentence=contract(corpusSentence)\n",
    "    corpusSentence=lowercase(corpusSentence)\n",
    "#     tokenized=regTokenize(corpusSentence)\n",
    "    \n",
    "    doc = nlp(corpusSentence)\n",
    "#     for i in range(0,5):\n",
    "#         print(doc[i].pos_,doc[i].lemma_)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def posPattern(corpusSentence):\n",
    "    doc = nlp(corpusSentence)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# def namedEntities(corpusSentence):\n",
    "#     corpusSentence=contract(corpusSentence)\n",
    "#     corpusSentence=lowercase(corpusSentence)\n",
    "# #     tokenized=regTokenize(corpusSentence)\n",
    "    \n",
    "#     doc = nlp(corpusSentence)\n",
    "# #     for i in range(0,5):\n",
    "# #         print(doc[i].pos_,doc[i].lemma_)\n",
    "#     return doc\n",
    "\n",
    "\n",
    "# -----------------------------------------POS Pattern-----------------------------------------\n",
    "def getPosPattern(data_sentences):\n",
    "    sentences_pos=[]\n",
    "    for i in data_sentences:\n",
    "        sent=[]\n",
    "        s=postagging(i)\n",
    "        for j in s:\n",
    "            sent.append(j.pos_)\n",
    "        sentences_pos.append(sent)\n",
    "    return sentences_pos\n",
    "\n",
    "def posGrams(sentence):\n",
    "    \n",
    "    trigram=list(ngrams(sentence,3))\n",
    "    fourgram=list(ngrams(sentence,4))\n",
    "    \n",
    "#     print('uni:',len(uni),' bi:',len(bigram),'tri:',len(trigram))\n",
    "    grams=[trigram,fourgram]\n",
    "    \n",
    "    return grams\n",
    "   \n",
    "def get34grams(sentences_pos):\n",
    "    \n",
    "    three_four_grams=[]\n",
    "    \n",
    "    for sent_pos in sentences_pos:\n",
    "        temp=posGrams(sent_pos)\n",
    "        temp = list(chain(*temp))\n",
    "#         print(temp)\n",
    "        three_four_grams.append(temp)\n",
    "    \n",
    "    return three_four_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11112  sentences\n",
      "8889   2223   8889   2223\n"
     ]
    }
   ],
   "source": [
    "dataset=joblib.load('dataset_with_labels.sav')\n",
    "\n",
    "print(len(dataset),' sentences')\n",
    "\n",
    "# print(np.array(dataset)[:,0])\n",
    "X=np.array(dataset)[:,0] # sentences\n",
    "Y=np.array(dataset)[:,1] # labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(len(X_train),' ',len(X_test),' ',len(y_train),' ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4989   6123\n"
     ]
    }
   ],
   "source": [
    "dataset_fact=[]\n",
    "dataset_nonfact=[]\n",
    "\n",
    "for i in dataset:\n",
    "    if(i[1]==1):\n",
    "        dataset_fact.append(i)\n",
    "    else:\n",
    "        dataset_nonfact.append(i)\n",
    "\n",
    "print(len(dataset_fact),' ' ,len(dataset_nonfact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
